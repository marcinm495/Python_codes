{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a construction by hand of a deep neural network for the tytanic classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries and modules:\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\envs\\tensorflow\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# This is a path to the python version used by this session. \n",
    "# In case of import error install lacking packages in the path below from a terminal using pip install Package_name\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of the test set:\n",
    "test_size_num = 0.3\n",
    "\n",
    "#Import data:\n",
    "data_file = pd.read_csv(\"train.csv\")\n",
    "\n",
    "#preparation of feature columns:\n",
    "\n",
    "data_file['age_num']=data_file['Age'].map(lambda x: data_file['Age'].median() if math.isnan(x) else x)\n",
    "data_file['sex_num']=data_file['Sex'].map(lambda x: 1 if x == 'male' else 0)\n",
    "data_file['emb_num']=data_file['Embarked'].map(lambda x: 1 if x==\"C\" else 2 if x==\"S\" else 3)\n",
    "\n",
    "def input_data(test_size_num, *feature_names):\n",
    "    return train_test_split(data_file[list(feature_names)].values, data_file['Survived'].values, \n",
    "                            test_size = test_size_num, random_state = 0)\n",
    "features_train, features_test, labels_train, labels_test =input_data(test_size_num, 'age_num', \n",
    "                                                                     'sex_num', 'emb_num', 'Parch', 'SibSp', 'Fare') \n",
    "#normalize the data:\n",
    "scaler = StandardScaler()\n",
    "features_train = scaler.fit_transform(features_train)\n",
    "features_test = scaler.transform(features_test)\n",
    "#label columns have to be reshaped into an array, with 0 -> [1,0] and 1 -> [0,1]:\n",
    "\n",
    "labels_train = (np.arange(2)==labels_train[:,None]).astype(np.float32)\n",
    "labels_test = (np.arange(2)==labels_test[:,None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-layer DNN with Gradient Descent optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of a singl-layer DNN, using Gradient Descent:\n",
    "num_labels = 2\n",
    "h_layer = 12\n",
    "n_Hlayers = 2\n",
    "f_size = features_train.shape[1]\n",
    "\n",
    "#Regularization parameters:\n",
    "\n",
    "\n",
    "def lin_op(x, w, b):\n",
    "    return tf.matmul(x,w)+b\n",
    "\n",
    "def dnn_h1(x, w_list, b_list):\n",
    "    l=len(w_list)\n",
    "    y_in = lin_op(x, w_list[0], b_list[0])\n",
    "    y_out = y_in\n",
    "    for k in range(1,l):\n",
    "        y_mid = tf.nn.relu(y_out)\n",
    "        y_out = lin_op(y_mid, w_list[k], b_list[k])\n",
    "    return y_out\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  #the data are put into constant tensors:\n",
    "  tf_train_dataset = tf.constant(features_train, dtype = tf.float32)\n",
    "  tf_train_labels = tf.constant(labels_train, dtype = tf.float32)\n",
    "  tf_test_dataset = tf.constant(features_test, dtype = tf.float32)\n",
    "  reg_param_w1 = tf.constant(0.01, dtype = tf.float32)\n",
    "  reg_param_w2 = tf.constant(0.01, dtype = tf.float32)\n",
    "    \n",
    "  #initialization of weight matrices for DNN with random normal inputs:\n",
    "  #ALL VALUES ARE float64 TYPE:\n",
    "    \n",
    "  w_list = []\n",
    "  b_list = []\n",
    "  if (n_Hlayers == 0):\n",
    "    w_list.append(tf.Variable(tf.truncated_normal([f_size, num_labels])))\n",
    "    b_list.append(tf.Variable(tf.zeros([num_labels])))\n",
    "  elif (n_Hlayers == 1):\n",
    "    w_list.append(tf.Variable(tf.truncated_normal([f_size, h_layer])))\n",
    "    b_list.append(tf.Variable(tf.zeros([h_layer])))\n",
    "    w_list.append(tf.Variable(tf.truncated_normal([h_layer, num_labels])))\n",
    "    b_list.append(tf.Variable(tf.zeros([num_labels])))\n",
    "  else:\n",
    "    w_list.append(tf.Variable(tf.truncated_normal([f_size, h_layer])))\n",
    "    b_list.append(tf.Variable(tf.zeros([h_layer])))\n",
    "    for k in range(1,n_Hlayers):\n",
    "        w_list.append(tf.Variable(tf.truncated_normal([h_layer, h_layer])))\n",
    "        b_list.append(tf.Variable(tf.zeros([h_layer])))\n",
    "    w_list.append(tf.Variable(tf.truncated_normal([h_layer, num_labels])))\n",
    "    b_list.append(tf.Variable(tf.zeros([num_labels])))\n",
    "     \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = dnn_h1(tf_train_dataset, w_list, b_list)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + (reg_param_w1 * tf.nn.l2_loss(w_list[0]))+ (reg_param_w2 * tf.nn.l2_loss(w_list[1]))\n",
    "  \n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss, var_list = (w_list, b_list))\n",
    "\n",
    "\n",
    "  test_prediction = tf.nn.softmax(dnn_h1(tf_test_dataset, w_list, b_list))\n",
    "  \n",
    "  # Predictions for the training and test data, based on optimized weight and bias matrices:\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  test_prediction = tf.nn.softmax(dnn_h1(tf_test_dataset, w_list, b_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 3.344238\n",
      "Training accuracy: 65.5%\n",
      "Test accuracy: 63.8%\n",
      "Loss at step 10: 1.184059\n",
      "Training accuracy: 80.7%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 20: 1.068396\n",
      "Training accuracy: 81.7%\n",
      "Test accuracy: 81.0%\n",
      "Loss at step 30: 0.994367\n",
      "Training accuracy: 82.2%\n",
      "Test accuracy: 80.6%\n",
      "Loss at step 40: 0.934824\n",
      "Training accuracy: 82.5%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 50: 0.882934\n",
      "Training accuracy: 82.8%\n",
      "Test accuracy: 79.9%\n",
      "Loss at step 60: 0.837448\n",
      "Training accuracy: 83.1%\n",
      "Test accuracy: 79.9%\n",
      "Loss at step 70: 0.796717\n",
      "Training accuracy: 83.3%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 80: 0.759738\n",
      "Training accuracy: 83.3%\n",
      "Test accuracy: 79.9%\n",
      "Loss at step 90: 0.726761\n",
      "Training accuracy: 83.1%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 100: 0.697098\n",
      "Training accuracy: 83.1%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 110: 0.670544\n",
      "Training accuracy: 83.5%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 120: 0.646727\n",
      "Training accuracy: 83.5%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 130: 0.625532\n",
      "Training accuracy: 83.6%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 140: 0.606312\n",
      "Training accuracy: 83.8%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 150: 0.588854\n",
      "Training accuracy: 84.1%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 160: 0.573025\n",
      "Training accuracy: 84.3%\n",
      "Test accuracy: 79.9%\n",
      "Loss at step 170: 0.558825\n",
      "Training accuracy: 84.1%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 180: 0.545763\n",
      "Training accuracy: 84.3%\n",
      "Test accuracy: 79.9%\n",
      "Loss at step 190: 0.534100\n",
      "Training accuracy: 84.3%\n",
      "Test accuracy: 79.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 200\n",
    "\n",
    "# accuracy calculated with the 'one-hot' encoding, namely output probabilities are mapped as  follows:\n",
    "#[0.2, 0.6, 0.2]->[0,1,0] etc.\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  #Initialization of all global variables:\n",
    "  tf.global_variables_initializer().run()\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 10 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(predictions, labels_train))\n",
    "      print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
